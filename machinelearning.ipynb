{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machinelearning",
      "provenance": [],
      "mount_file_id": "1S_nD6BKJltzNp0_kcUBl8u5PxvYpAqL_",
      "authorship_tag": "ABX9TyO4TDJT9VSvHjteacLab82w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2b03204b7f264f2c9fcedbad0afe0419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_54a32406ddb74f818cb3051c031ba2bd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_15ff7de1227548709d94443ac811f65a",
              "IPY_MODEL_5d3ec5ea18b241fe9d8c3af06913638a"
            ]
          }
        },
        "54a32406ddb74f818cb3051c031ba2bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "15ff7de1227548709d94443ac811f65a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d0dab1ed28c84612aa91b8d3c07cec06",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b79db29520104216a7b71a65ccdd713c"
          }
        },
        "5d3ec5ea18b241fe9d8c3af06913638a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b70d56185aae47bf85bb75c0a59101ef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [02:36&lt;00:00, 3.54MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9f07b8c49f6f497889272ed25eec1c42"
          }
        },
        "d0dab1ed28c84612aa91b8d3c07cec06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b79db29520104216a7b71a65ccdd713c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b70d56185aae47bf85bb75c0a59101ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9f07b8c49f6f497889272ed25eec1c42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shishc9/blogSystem-backend/blob/master/machinelearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0lRbET6i9G2",
        "outputId": "ae596167-254a-4ab0-b057-b15d3ac1e08b"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "\n",
        "trainpath = '/content/drive/MyDrive/Machine/train'\n",
        "validationpath = '/content/drive/MyDrive/Machine/validation'\n",
        "num_sample = 30    #定义随机采样数量 \n",
        "if not os.path.exists(validationpath):\n",
        "    os.mkdir(validationpath)\n",
        "else:\n",
        "    print('error path already exists!!')\n",
        "\n",
        "folderlist = os.listdir(trainpath)\n",
        "print('folder list : {}'.format(folderlist))\n",
        "for dirpath in folderlist:\n",
        "    if not os.path.exists(os.path.join(validationpath, dirpath)):\n",
        "        os.mkdir(os.path.join(validationpath, dirpath))\n",
        "    loadpath = os.path.join(trainpath,dirpath)\n",
        "    datalist = os.listdir(loadpath)\n",
        "    samplelist = random.sample(datalist, num_sample)\n",
        "    for filename in samplelist:\n",
        "        os.renames(os.path.join(loadpath, filename), os.path.join(validationpath, dirpath, filename))\n",
        "\n",
        "\n",
        "print('Sampling completed')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error path already exists!!\n",
            "folder list : ['sea', 'ship']\n",
            "Sampling completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490,
          "referenced_widgets": [
            "2b03204b7f264f2c9fcedbad0afe0419",
            "54a32406ddb74f818cb3051c031ba2bd",
            "15ff7de1227548709d94443ac811f65a",
            "5d3ec5ea18b241fe9d8c3af06913638a",
            "d0dab1ed28c84612aa91b8d3c07cec06",
            "b79db29520104216a7b71a65ccdd713c",
            "b70d56185aae47bf85bb75c0a59101ef",
            "9f07b8c49f6f497889272ed25eec1c42"
          ]
        },
        "id": "IFjnP5H4jGO9",
        "outputId": "5311e853-2b64-4e96-ee8d-aa100914597a"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as dset\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import copy\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "if use_gpu:\n",
        "    print(\"Using CUDA\")\n",
        "\n",
        "\n",
        "# device check\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('runing device : {}'.format(device))\n",
        "\n",
        "\n",
        "# Training settings\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "train_set = dset.ImageFolder(root='/content/drive/MyDrive/Machine/train',transform = transforms.Compose([\n",
        "                                transforms.RandomResizedCrop(224),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor()\n",
        "                               ]))\n",
        "\n",
        "validation_set = dset.ImageFolder(root='/content/drive/MyDrive/Machine/validation',transform = transforms.Compose([\n",
        "                                transforms.Resize(256),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor()]))\n",
        "\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_set,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "# Load the pretrained model from pytorch\n",
        "# vgg16 = models.vgg16(pretrained=True)\n",
        "### 当pretrained=True时，运行时会自动下载vgg16-397923af.pth这个预训练模型文件到你的电脑，下载完成后只需要将下载的模型文件按照路径加载进来就可以使用了，需要注意，如果使用预加载训练模型时，需要将trianed=False\n",
        "vgg16 = models.vgg16(pretrained=False)\n",
        "vgg16.load_state_dict(torch.load(\"/root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\"))\n",
        "print(vgg16.classifier[6].out_features) # 1000\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Freeze training for all layers\n",
        "for param in vgg16.features.parameters():\n",
        "    param.require_grad = False\n",
        "\n",
        "\n",
        "# Newly created modules have require_grad=True by default\n",
        "num_features = vgg16.classifier[6].in_features\n",
        "features = list(vgg16.classifier.children())[:-1] # Remove last layer\n",
        "# 我这里是2类目标，所以应该是2\n",
        "features.extend([nn.Linear(num_features, 2)]) # Add our layer with 2 outputs\n",
        "features.extend([nn.LogSoftmax(1)])\n",
        "vgg16.classifier = nn.Sequential(*features) # Replace the model classifier\n",
        "\n",
        "\n",
        "\n",
        "# If you want to train the model for more than 2 epochs, set this to True after the first run\n",
        "resume_training = False\n",
        "\n",
        "if resume_training:\n",
        "    print(\"Loading pretrained model..\")\n",
        "    vgg16.load_state_dict(torch.load('./vgg16-397923af.pth'))\n",
        "    print(\"Loaded!\")\n",
        "\n",
        "vgg16.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_ft = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "def train(vgg, optimizer, epoch, num_epochs=10):\n",
        "    vgg.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = vgg(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            # print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            #     num_epochs, batch_idx * len(data), len(train_loader.dataset),\n",
        "            #     100. * batch_idx / len(train_loader), loss.data[0]))\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch*num_epochs+batch_idx, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.tolist()))\n",
        "\n",
        "\n",
        "def test(vgg):\n",
        "    vgg.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in validation_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        output = vgg(data)\n",
        "        # sum up batch loss\n",
        "        # test_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).tolist()\n",
        "        # get the index of the max log-probability\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(validation_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(validation_loader.dataset),\n",
        "        100. * correct / len(validation_loader.dataset)))\n",
        "    return correct\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(5):\n",
        "    train(vgg16, optimizer_ft, epoch)\n",
        "    torch.save(vgg16, 'best#111.pb')\n",
        "test(vgg16)\n",
        "torch.save(vgg16, 'best#111.pb')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CUDA\n",
            "runing device : cuda:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b03204b7f264f2c9fcedbad0afe0419",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-08a546711e8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m### 当pretrained=True时，运行时会自动下载vgg16-397923af.pth这个预训练模型文件到你的电脑，下载完成后只需要将下载的模型文件按照路径加载进来就可以使用了，需要注意，如果使用预加载训练模型时，需要将trianed=False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mvgg16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mvgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vgg16-397923af.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vgg16-397923af.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDcgRb1KjoTg",
        "outputId": "3b6619df-4c4d-478f-c08a-271f8047e9f1"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as dset\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import copy\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "if use_gpu:\n",
        "    print(\"Using CUDA\")\n",
        "\n",
        "\n",
        "# device check\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('runing device : {}'.format(device))\n",
        "\n",
        "\n",
        "# Training settings\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "train_set = dset.ImageFolder(root='/content/drive/MyDrive/Machine/train',transform = transforms.Compose([\n",
        "                                transforms.RandomResizedCrop(224),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor()\n",
        "                               ]))\n",
        "\n",
        "validation_set = dset.ImageFolder(root='/content/drive/MyDrive/Machine/validation',transform = transforms.Compose([\n",
        "                                transforms.Resize(256),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor()]))\n",
        "\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_set,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "# Load the pretrained model from pytorch\n",
        "# vgg16 = models.vgg16(pretrained=True)\n",
        "### 当pretrained=True时，运行时会自动下载vgg16-397923af.pth这个预训练模型文件到你的电脑，下载完成后只需要将下载的模型文件按照路径加载进来就可以使用了，需要注意，如果使用预加载训练模型时，需要将trianed=False\n",
        "vgg16 = models.vgg16(pretrained=False)\n",
        "vgg16.load_state_dict(torch.load(\"/root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\"))\n",
        "print(vgg16.classifier[6].out_features) # 1000\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Freeze training for all layers\n",
        "for param in vgg16.features.parameters():\n",
        "    param.require_grad = False\n",
        "\n",
        "\n",
        "# Newly created modules have require_grad=True by default\n",
        "num_features = vgg16.classifier[6].in_features\n",
        "features = list(vgg16.classifier.children())[:-1] # Remove last layer\n",
        "# 我这里是2类目标，所以应该是2\n",
        "features.extend([nn.Linear(num_features, 2)]) # Add our layer with 2 outputs\n",
        "features.extend([nn.LogSoftmax(1)])\n",
        "vgg16.classifier = nn.Sequential(*features) # Replace the model classifier\n",
        "\n",
        "\n",
        "\n",
        "# If you want to train the model for more than 2 epochs, set this to True after the first run\n",
        "resume_training = False\n",
        "\n",
        "if resume_training:\n",
        "    print(\"Loading pretrained model..\")\n",
        "    vgg16.load_state_dict(torch.load('./vgg16-397923af.pth'))\n",
        "    print(\"Loaded!\")\n",
        "\n",
        "vgg16.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_ft = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "def train(vgg, optimizer, epoch, num_epochs=10):\n",
        "    vgg.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = vgg(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            # print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            #     num_epochs, batch_idx * len(data), len(train_loader.dataset),\n",
        "            #     100. * batch_idx / len(train_loader), loss.data[0]))\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch*num_epochs+batch_idx, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.tolist()))\n",
        "\n",
        "\n",
        "def test(vgg):\n",
        "    vgg.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in validation_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        output = vgg(data)\n",
        "        # sum up batch loss\n",
        "        # test_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).tolist()\n",
        "        # get the index of the max log-probability\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(validation_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(validation_loader.dataset),\n",
        "        100. * correct / len(validation_loader.dataset)))\n",
        "    return correct\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(5):\n",
        "    train(vgg16, optimizer_ft, epoch)\n",
        "    torch.save(vgg16, 'best#111.pb')\n",
        "test(vgg16)\n",
        "torch.save(vgg16, 'best#111.pb')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CUDA\n",
            "runing device : cuda:0\n",
            "1000\n",
            "Train Epoch: 0 [0/2580 (0%)]\tLoss: 0.796237\n",
            "Train Epoch: 10 [320/2580 (12%)]\tLoss: 0.483265\n",
            "Train Epoch: 20 [640/2580 (25%)]\tLoss: 0.405360\n",
            "Train Epoch: 30 [960/2580 (37%)]\tLoss: 0.105630\n",
            "Train Epoch: 40 [1280/2580 (49%)]\tLoss: 0.206417\n",
            "Train Epoch: 50 [1600/2580 (62%)]\tLoss: 0.358646\n",
            "Train Epoch: 60 [1920/2580 (74%)]\tLoss: 0.155632\n",
            "Train Epoch: 70 [2240/2580 (86%)]\tLoss: 0.213767\n",
            "Train Epoch: 80 [1600/2580 (99%)]\tLoss: 0.044564\n",
            "Train Epoch: 10 [0/2580 (0%)]\tLoss: 0.226952\n",
            "Train Epoch: 20 [320/2580 (12%)]\tLoss: 0.251693\n",
            "Train Epoch: 30 [640/2580 (25%)]\tLoss: 0.370372\n",
            "Train Epoch: 40 [960/2580 (37%)]\tLoss: 0.404641\n",
            "Train Epoch: 50 [1280/2580 (49%)]\tLoss: 0.222951\n",
            "Train Epoch: 60 [1600/2580 (62%)]\tLoss: 0.115628\n",
            "Train Epoch: 70 [1920/2580 (74%)]\tLoss: 0.278385\n",
            "Train Epoch: 80 [2240/2580 (86%)]\tLoss: 0.131753\n",
            "Train Epoch: 90 [1600/2580 (99%)]\tLoss: 0.163975\n",
            "Train Epoch: 20 [0/2580 (0%)]\tLoss: 0.200315\n",
            "Train Epoch: 30 [320/2580 (12%)]\tLoss: 0.025499\n",
            "Train Epoch: 40 [640/2580 (25%)]\tLoss: 0.363816\n",
            "Train Epoch: 50 [960/2580 (37%)]\tLoss: 0.037462\n",
            "Train Epoch: 60 [1280/2580 (49%)]\tLoss: 0.185101\n",
            "Train Epoch: 70 [1600/2580 (62%)]\tLoss: 0.169595\n",
            "Train Epoch: 80 [1920/2580 (74%)]\tLoss: 0.479702\n",
            "Train Epoch: 90 [2240/2580 (86%)]\tLoss: 0.092012\n",
            "Train Epoch: 100 [1600/2580 (99%)]\tLoss: 0.098877\n",
            "Train Epoch: 30 [0/2580 (0%)]\tLoss: 0.089303\n",
            "Train Epoch: 40 [320/2580 (12%)]\tLoss: 0.171780\n",
            "Train Epoch: 50 [640/2580 (25%)]\tLoss: 0.112434\n",
            "Train Epoch: 60 [960/2580 (37%)]\tLoss: 0.164303\n",
            "Train Epoch: 70 [1280/2580 (49%)]\tLoss: 0.213803\n",
            "Train Epoch: 80 [1600/2580 (62%)]\tLoss: 0.109114\n",
            "Train Epoch: 90 [1920/2580 (74%)]\tLoss: 0.222775\n",
            "Train Epoch: 100 [2240/2580 (86%)]\tLoss: 0.191232\n",
            "Train Epoch: 110 [1600/2580 (99%)]\tLoss: 0.455228\n",
            "Train Epoch: 40 [0/2580 (0%)]\tLoss: 0.118788\n",
            "Train Epoch: 50 [320/2580 (12%)]\tLoss: 0.099391\n",
            "Train Epoch: 60 [640/2580 (25%)]\tLoss: 0.137833\n",
            "Train Epoch: 70 [960/2580 (37%)]\tLoss: 0.138319\n",
            "Train Epoch: 80 [1280/2580 (49%)]\tLoss: 0.105022\n",
            "Train Epoch: 90 [1600/2580 (62%)]\tLoss: 0.158080\n",
            "Train Epoch: 100 [1920/2580 (74%)]\tLoss: 0.039683\n",
            "Train Epoch: 110 [2240/2580 (86%)]\tLoss: 0.141315\n",
            "Train Epoch: 120 [1600/2580 (99%)]\tLoss: 0.126028\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1000, Accuracy: 687/716 (96%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZD-oCLyOret-",
        "outputId": "5857654f-c3e1-4813-d895-c2a30db249fd"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "from torchvision.transforms import RandomCrop, Resize, Compose, ToTensor\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "imagesize = (224,224,3)\n",
        "labeldict = {'0' : 'sea', '1' : 'ship'}\n",
        "\n",
        "def predict(model, imgarr, device):\n",
        "    inimage = cv2.cvtColor(imgarr, cv2.COLOR_BGR2RGB)\n",
        "    inimage = cv2.resize(inimage, (imagesize[0], imagesize[1]))\n",
        "    inimage = np.transpose(inimage, (2, 0, 1)) / 255\n",
        "    inimage = np.expand_dims(inimage, axis=0)\n",
        "    inimage = torch.tensor(inimage.astype('float32'))\n",
        "    inimage = inimage.to(device)\n",
        "    netout = model(torch.tensor(inimage))\n",
        "    label = str(netout.argmax().item())\n",
        "    return labeldict[label]\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('runing device : {}'.format(device))\n",
        "model = torch.load('./best#111.pb')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "testdir = '/content/drive/MyDrive/Machine/test/ship'\n",
        "for filename in os.listdir(testdir):\n",
        "    filepath = os.path.join(testdir, filename)\n",
        "    img = cv2.imread(filepath)\n",
        "    #rec_c = cv2.cvtColor(rec_c, cv2.COLOR)\n",
        "    # print(img.shape)\n",
        "    sol = predict(model, img, device)\n",
        "    print(filename +'prediction : {}'.format(sol))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "runing device : cuda:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ship__20180705_182236_0f4d__-122.3334438500883_37.75037217054128.pngprediction : ship\n",
            "ship__20171118_181532_1030__-122.34400966135502_37.75705610175418.pngprediction : ship\n",
            "ship__20171118_185722_0f2d__-122.34496788529589_37.73721449905369.pngprediction : ship\n",
            "ship__20171025_175648_0e26__-118.15349753221352_33.69822802602944.pngprediction : ship\n",
            "ship__20171217_181637_1032__-122.33377734078427_37.728363954110335.pngprediction : ship\n",
            "ship__20171118_185723_0f2d__-122.35675214597191_37.79184481316763.pngprediction : sea\n",
            "ship__20171023_175305_1039__-118.11559165475667_33.65552976108269.pngprediction : ship\n",
            "ship__20180205_181812_1030__-122.32540372847062_37.72823521408221.pngprediction : ship\n",
            "ship__20171023_175305_1039__-118.10923630341375_33.67963996316044.pngprediction : ship\n",
            "ship__20171127_181539_101e__-122.33856455895739_37.71068001749797.pngprediction : ship\n",
            "ship__20180312_182016_1042__-122.33736834649797_37.753260475183126.pngprediction : ship\n",
            "ship__20171129_181545_1022__-122.342773203759_37.7505327212711.pngprediction : ship\n",
            "ship__20171127_181539_101e__-122.32971449979756_37.71076523356743.pngprediction : ship\n",
            "ship__20171129_181544_1022__-122.33933442316871_37.769603277117824.pngprediction : sea\n",
            "ship__20171217_181637_1032__-122.3214077502019_37.71845102201261.pngprediction : ship\n",
            "ship__20171107_181615_0f22__-122.33216979337155_37.74814772819181.pngprediction : ship\n",
            "ship__20180705_213444_0f02__-122.3287178577925_37.73823169303052.pngprediction : ship\n",
            "ship__20171118_185722_0f2d__-122.32899364726843_37.737854711217835.pngprediction : ship\n",
            "ship__20171119_181613_0f52__-122.34016581020008_37.71817894771884.pngprediction : ship\n",
            "ship__20171118_185722_0f2d__-122.34586056213097_37.759581050926116.pngprediction : ship\n",
            "ship__20180705_182235_0f4d__-122.33246558217941_37.71900086396819.pngprediction : ship\n",
            "ship__20171213_181610_100c__-122.35706399409156_37.75977790123511.pngprediction : ship\n",
            "ship__20180705_182235_0f4d__-122.34741817434761_37.71794891969327.pngprediction : ship\n",
            "ship__20171025_175647_0e26__-118.14347911930842_33.73362914920917.pngprediction : ship\n",
            "ship__20180705_182235_0f4d__-122.32525004003183_37.70014025279344.pngprediction : ship\n",
            "ship__20171212_181755_0e26__-122.32982847634631_37.73967110277111.pngprediction : ship\n",
            "ship__20180312_182016_1042__-122.34271012170575_37.74996454207846.pngprediction : ship\n",
            "ship__20171207_181551_103c__-122.34773434894691_37.76727061134796.pngprediction : ship\n",
            "ship__20180203_181746_1035__-122.32169237663439_37.71710068406658.pngprediction : ship\n",
            "ship__20171127_181538_101e__-122.34280869089645_37.7503706827579.pngprediction : ship\n",
            "ship__20171212_181756_0e26__-122.34197035081759_37.72197456876908.pngprediction : ship\n",
            "ship__20171023_184815_0f21__-118.21694562064921_33.73755340598831.pngprediction : ship\n",
            "ship__20170923_181241_0f42__-122.34794155805267_37.76694728611726.pngprediction : ship\n",
            "ship__20171119_181612_0f52__-122.3417050874871_37.74809321393093.pngprediction : ship\n",
            "ship__20171107_181615_0f22__-122.33764289329143_37.73774139431109.pngprediction : ship\n",
            "ship__20180203_181746_1035__-122.3238523014483_37.72663099646809.pngprediction : ship\n",
            "ship__20171118_185722_0f2d__-122.33606293980446_37.7583365292858.pngprediction : ship\n",
            "ship__20180206_181845_0f34__-122.34587818956939_37.76144686447373.pngprediction : ship\n",
            "ship__20171023_175304_1039__-118.21103492453648_33.72944073523736.pngprediction : ship\n",
            "ship__20171022_175534_100e__-118.1950089042915_33.71092843328897.pngprediction : ship\n",
            "ship__20171023_175304_1039__-118.16932998870766_33.73433398479186.pngprediction : ship\n",
            "ship__20171129_181545_1022__-122.32926284720676_37.738207756054706.pngprediction : ship\n",
            "ship__20180206_181845_0f34__-122.32496339153784_37.72798933809468.pngprediction : ship\n",
            "ship__20171118_185722_0f2d__-122.34006393759125_37.71815133796416.pngprediction : ship\n",
            "ship__20180305_184041_1054__-122.33639427812646_37.73649055664562.pngprediction : ship\n",
            "ship__20171217_181637_1032__-122.34338830121405_37.75799901938855.pngprediction : ship\n",
            "ship__20171207_181551_103c__-122.34767948512105_37.73860854907357.pngprediction : ship\n",
            "ship__20171107_181615_0f22__-122.32523744384153_37.72780163075094.pngprediction : ship\n",
            "ship__20171129_181545_1022__-122.34076267101332_37.719966896346506.pngprediction : ship\n",
            "ship__20171217_181637_1032__-122.34068235231403_37.74822269423428.pngprediction : ship\n",
            "ship__20170925_181400_103e__-122.35138059582_37.74757902961195.pngprediction : ship\n",
            "ship__20171119_181613_0f52__-122.33513975046789_37.72829052754263.pngprediction : ship\n",
            "ship__20180705_182235_0f4d__-122.33513153134916_37.72920982375914.pngprediction : ship\n",
            "ship__20180210_181908_1006__-122.34714211397687_37.76102124348205.pngprediction : ship\n",
            "ship__20180210_181908_1006__-122.35282451856958_37.78509053291386.pngprediction : ship\n",
            "ship__20180210_181908_1006__-122.35609704801801_37.76131386622846.pngprediction : ship\n",
            "ship__20180705_182235_0f4d__-122.33864576415738_37.73982906283797.pngprediction : ship\n",
            "ship__20171118_185722_0f2d__-122.33510401615905_37.728479602418275.pngprediction : ship\n",
            "ship__20180205_181811_1030__-122.39782230108867_37.798740586411384.pngprediction : ship\n",
            "ship__20171023_175304_1039__-118.21728459759943_33.7363325640949.pngprediction : ship\n",
            "ship__20171118_181531_1030__-122.34807281296018_37.76751583826101.pngprediction : ship\n",
            "ship__20180305_184041_1054__-122.34522826530379_37.75790109894697.pngprediction : ship\n",
            "ship__20180705_182235_0f4d__-122.32430799752129_37.72893196483871.pngprediction : ship\n",
            "ship__20171023_184815_0f21__-118.22656678110803_33.72184714169161.pngprediction : sea\n",
            "ship__20171129_181545_1022__-122.33893028610122_37.71165549329901.pngprediction : ship\n",
            "ship__20171025_175648_0e26__-118.22893579949042_33.71995675330757.pngprediction : ship\n",
            "ship__20171217_181637_1032__-122.36206729178167_37.77205391869949.pngprediction : ship\n",
            "ship__20171129_181544_1022__-122.3364672628105_37.758825514797536.pngprediction : ship\n",
            "ship__20170923_181241_0f42__-122.33785767749494_37.7671344111478.pngprediction : sea\n",
            "ship__20180205_181812_1030__-122.33525851672547_37.73023804026718.pngprediction : ship\n",
            "ship__20180705_182236_0f4d__-122.35012563648382_37.770366306264116.pngprediction : ship\n",
            "ship__20171118_185722_0f2d__-122.34573065944548_37.72791715264537.pngprediction : ship\n",
            "ship__20180209_184353_0f21__-122.32506134542359_37.728449570059034.pngprediction : ship\n",
            "ship__20180312_182016_1042__-122.33950343558382_37.73910380697799.pngprediction : ship\n",
            "ship__20180210_181908_1006__-122.34356739465089_37.74929333475811.pngprediction : ship\n",
            "ship__20171118_181532_1030__-122.33067957924183_37.747246974838895.pngprediction : sea\n",
            "ship__20171217_181637_1032__-122.33811665447881_37.73806852894704.pngprediction : ship\n",
            "ship__20170925_181359_103e__-122.34809524238744_37.76884088933075.pngprediction : ship\n",
            "ship__20171213_181610_100c__-122.33323827337007_37.72014089670815.pngprediction : ship\n",
            "ship__20171023_181359_1044__-122.34396930188824_37.72690690196969.pngprediction : ship\n",
            "ship__20171118_181532_1030__-122.33975808025951_37.71809554637377.pngprediction : ship\n",
            "ship__20171212_181756_0e26__-122.33464550132062_37.73026162128987.pngprediction : sea\n",
            "ship__20170925_181359_103e__-122.33816349367736_37.76721724705639.pngprediction : ship\n",
            "ship__20171025_175648_0e26__-118.22012388897608_33.72415658375151.pngprediction : ship\n",
            "ship__20180131_175841_1010__-118.2112386851259_33.73011512692281.pngprediction : ship\n",
            "ship__20180205_181812_1030__-122.34903070619661_37.73983279484131.pngprediction : ship\n",
            "ship__20170923_181241_0f42__-122.33247965400024_37.747770931971786.pngprediction : ship\n",
            "ship__20171025_175648_0e26__-118.20729510768273_33.70285456443248.pngprediction : ship\n",
            "ship__20180131_175841_1010__-118.21918197635038_33.72394944212786.pngprediction : ship\n",
            "ship__20171023_175305_1039__-118.1167284926536_33.688419843356876.pngprediction : ship\n",
            "ship__20171118_181532_1030__-122.34627580657862_37.72786609802154.pngprediction : ship\n",
            "ship__20171213_181610_100c__-122.34241525126555_37.72170665596761.pngprediction : ship\n",
            "ship__20180205_181811_1030__-122.35099970507248_37.75220067254825.pngprediction : ship\n",
            "ship__20171023_184815_0f21__-118.221018877267_33.732480856222665.pngprediction : sea\n",
            "ship__20180705_182235_0f4d__-122.33019982898477_37.740024726163895.pngprediction : ship\n",
            "ship__20171022_175534_100e__-118.21807781545026_33.72371683818665.pngprediction : ship\n",
            "ship__20171023_175305_1039__-118.22109531451522_33.731019099726836.pngprediction : ship\n",
            "ship__20171022_175534_100e__-118.22693678438137_33.7205447978323.pngprediction : ship\n",
            "ship__20171217_181637_1032__-122.32746155880102_37.73784597961478.pngprediction : ship\n",
            "ship__20171023_175304_1039__-118.16090423887094_33.733872379270466.pngprediction : sea\n",
            "ship__20171023_175305_1039__-118.1288766522932_33.70411018371466.pngprediction : ship\n",
            "ship__20171118_181532_1030__-122.34455575468905_37.73761780000452.pngprediction : ship\n",
            "ship__20171207_181551_103c__-122.33515165952372_37.757412053801005.pngprediction : ship\n",
            "ship__20180705_213443_0f02__-122.3458138794183_37.75714724870342.pngprediction : ship\n",
            "ship__20171127_181538_101e__-122.35285080613363_37.7859829726792.pngprediction : ship\n",
            "ship__20171129_181544_1022__-122.35145136405632_37.751175661392985.pngprediction : ship\n",
            "ship__20180205_181922_1031__-122.39075426085026_37.80462583780358.pngprediction : ship\n",
            "ship__20171117_185444_104c__-122.34558214564862_37.737055674168104.pngprediction : ship\n",
            "ship__20171022_175534_100e__-118.25263689798176_33.71111496385489.pngprediction : ship\n",
            "ship__20171213_181610_100c__-122.3464371435492_37.759854643523596.pngprediction : ship\n",
            "ship__20180705_213443_0f02__-122.3490293412872_37.76719665414427.pngprediction : ship\n",
            "ship__20180206_181845_0f34__-122.34885999520644_37.73988593083626.pngprediction : ship\n",
            "ship__20171117_185444_104c__-122.33091744750324_37.74730240420565.pngprediction : ship\n",
            "ship__20171023_175305_1039__-118.13450944820356_33.679491675322964.pngprediction : ship\n",
            "ship__20180206_181844_0f34__-122.3506999527115_37.770910258564356.pngprediction : ship\n",
            "ship__20171212_181755_0e26__-122.33266963436137_37.74936733203823.pngprediction : sea\n",
            "ship__20171129_181544_1022__-122.342773203759_37.7505327212711.pngprediction : ship\n",
            "ship__20171023_175305_1039__-118.13810150104705_33.696666298658606.pngprediction : sea\n",
            "ship__20171107_181615_0f22__-122.34861848678705_37.76743773780358.pngprediction : ship\n",
            "ship__20171217_181637_1032__-122.34731160755146_37.7688635908923.pngprediction : ship\n",
            "ship__20180131_175841_1010__-118.15357365643099_33.69665806917337.pngprediction : ship\n",
            "ship__20171213_181610_100c__-122.33867435049139_37.70981536694513.pngprediction : ship\n",
            "ship__20180705_182235_0f4d__-122.32885877198521_37.71130115669558.pngprediction : ship\n",
            "ship__20171129_181545_1022__-122.33949406596776_37.74015829279448.pngprediction : ship\n",
            "ship__20180705_182235_0f4d__-122.32053495342223_37.70965814653878.pngprediction : ship\n",
            "ship__20171129_181544_1022__-122.34815743017721_37.769517216389204.pngprediction : sea\n",
            "ship__20171023_175304_1039__-118.22112769129514_33.731018779528924.pngprediction : ship\n",
            "ship__20171118_181531_1030__-122.34400966135502_37.75705610175418.pngprediction : ship\n",
            "ship__20180131_175841_1010__-118.21849064582784_33.736888833857726.pngprediction : ship\n",
            "ship__20171118_181532_1030__-122.32906247218433_37.737773984680445.pngprediction : sea\n",
            "ship__20171212_181755_0e26__-122.35002629223312_37.770041286507066.pngprediction : ship\n",
            "ship__20171129_181545_1022__-122.32579948546167_37.729643530806925.pngprediction : ship\n",
            "ship__20171118_185722_0f2d__-122.33805602474183_37.73722996549543.pngprediction : ship\n",
            "ship__20180312_182016_1042__-122.33270117893291_37.71929963254146.pngprediction : ship\n",
            "ship__20171022_175534_100e__-118.16932888763621_33.734252827962536.pngprediction : ship\n",
            "ship__20171107_181615_0f22__-122.35442425974563_37.757843674659476.pngprediction : ship\n",
            "ship__20180705_182236_0f4d__-122.35823748780594_37.79344822790265.pngprediction : ship\n",
            "ship__20180705_182236_0f4d__-122.33953638448827_37.769874802316046.pngprediction : ship\n",
            "ship__20180705_182236_0f4d__-122.34239097094304_37.75139584987269.pngprediction : ship\n",
            "ship__20171023_175305_1039__-118.22703351777623_33.720516781116096.pngprediction : ship\n",
            "ship__20180305_184041_1054__-122.3387514205941_37.76622009272989.pngprediction : ship\n",
            "ship__20180209_184353_0f21__-122.34943264991506_37.7405921187298.pngprediction : ship\n",
            "ship__20180705_182236_0f4d__-122.33599555315165_37.758255031566605.pngprediction : ship\n",
            "ship__20171127_181539_101e__-122.32593565600725_37.72964430953292.pngprediction : ship\n",
            "ship__20171217_181636_1032__-122.35747045451738_37.791524261982296.pngprediction : ship\n",
            "ship__20171118_185722_0f2d__-122.33681896411063_37.765208778414234.pngprediction : ship\n",
            "ship__20171118_181531_1030__-122.33589290227154_37.75830853186827.pngprediction : ship\n",
            "ship__20171022_175534_100e__-118.16499076011308_33.727151109873056.pngprediction : ship\n",
            "ship__20180131_175841_1010__-118.14847605898571_33.72708936752747.pngprediction : ship\n",
            "ship__20171119_181613_0f52__-122.3379879323536_37.73722958308728.pngprediction : ship\n",
            "ship__20171023_175304_1039__-118.14264050152555_33.73388041561904.pngprediction : ship\n",
            "ship__20171212_181755_0e26__-122.34291347769646_37.75007383360081.pngprediction : ship\n",
            "ship__20171023_181359_1044__-122.33295932479058_37.71708385178135.pngprediction : ship\n",
            "ship__20180705_182235_0f4d__-122.34236820873906_37.71932691449419.pngprediction : ship\n",
            "ship__20171212_181756_0e26__-122.33313373443386_37.720410700647555.pngprediction : ship\n",
            "ship__20180705_182236_0f4d__-122.34671030989746_37.75977504002937.pngprediction : ship\n",
            "ship__20171023_175305_1039__-118.19494678832497_33.711118424550534.pngprediction : ship\n",
            "ship__20180205_181811_1030__-122.36586298001781_37.762367278018836.pngprediction : ship\n",
            "ship__20180205_181812_1030__-122.32286914728564_37.718702780425694.pngprediction : ship\n",
            "ship__20171023_184815_0f21__-118.21766087329691_33.72629123298313.pngprediction : ship\n",
            "ship__20171023_184815_0f21__-118.19505626473541_33.71201020510068.pngprediction : ship\n",
            "ship__20171212_181755_0e26__-122.33732803969346_37.73868600531012.pngprediction : ship\n",
            "ship__20171118_185722_0f2d__-122.33071241242664_37.74738235827348.pngprediction : ship\n",
            "ship__20171129_181545_1022__-122.35145112805893_37.75120269950231.pngprediction : ship\n",
            "ship__20171127_181539_101e__-122.34707500910532_37.729898490055696.pngprediction : ship\n",
            "ship__20171207_181551_103c__-122.33535415235686_37.7271560778983.pngprediction : sea\n",
            "ship__20171119_181613_0f52__-122.33806559292786_37.70937931753214.pngprediction : sea\n",
            "ship__20171119_181612_0f52__-122.34558216091033_37.75638885974003.pngprediction : ship\n",
            "ship__20171022_175534_100e__-118.21592635116053_33.708911545121495.pngprediction : ship\n",
            "ship__20171022_175534_100e__-118.21728612601537_33.7364407712713.pngprediction : ship\n",
            "ship__20171210_181628_100a__-122.33374787980998_37.720251935906376.pngprediction : ship\n",
            "ship__20171119_181613_0f52__-122.34641363784725_37.72767758573592.pngprediction : ship\n",
            "ship__20180205_181922_1031__-122.36027037593409_37.77107076127549.pngprediction : ship\n",
            "ship__20180210_181908_1006__-122.34975403850396_37.77001274518038.pngprediction : ship\n",
            "ship__20171118_181532_1030__-122.33517282566321_37.72839887204015.pngprediction : ship\n",
            "ship__20180205_181811_1030__-122.34423745384966_37.750540877374135.pngprediction : sea\n",
            "ship__20180205_184449_104a__-122.32314587629772_37.718217658219075.pngprediction : sea\n",
            "ship__20171119_181612_0f52__-122.34803851543886_37.76754268784317.pngprediction : ship\n",
            "ship__20171129_181545_1022__-122.32961142336484_37.7108728057609.pngprediction : ship\n",
            "ship__20171217_181637_1032__-122.35089797724429_37.74825235823534.pngprediction : ship\n",
            "ship__20171118_181532_1030__-122.33856599599876_37.73731394661371.pngprediction : ship\n",
            "ship__20171026_184638_1043__-118.15404484785569_33.6980064665278.pngprediction : ship\n",
            "ship__20180312_182016_1042__-122.34746618475607_37.739634867823405.pngprediction : ship\n",
            "ship__20171207_181551_103c__-122.33142699267296_37.74744049268292.pngprediction : ship\n",
            "ship__20180206_181845_0f34__-122.33563154002478_37.73040238008357.pngprediction : ship\n",
            "ship__20180210_181908_1006__-122.34123392601467_37.77050620854762.pngprediction : ship\n",
            "ship__20171119_181612_0f52__-122.33606318146319_37.75830949129269.pngprediction : ship\n",
            "ship__20180312_182016_1042__-122.32669610024661_37.72837780013602.pngprediction : ship\n",
            "ship__20171025_175647_0e26__-118.22668998523704_33.728231032975344.pngprediction : ship\n",
            "ship__20171107_181615_0f22__-122.35043213027565_37.74700597830246.pngprediction : ship\n",
            "ship__20171025_175648_0e26__-118.1312075790471_33.70165381720052.pngprediction : ship\n",
            "ship__20171117_185444_104c__-122.34651124917606_37.728191877515975.pngprediction : ship\n",
            "ship__20171127_181538_101e__-122.34620667045475_37.739600848957714.pngprediction : ship\n",
            "ship__20171207_181551_103c__-122.32780740144239_37.7372530847441.pngprediction : ship\n",
            "ship__20171118_181531_1030__-122.33644310714187_37.76534185924881.pngprediction : ship\n",
            "ship__20170923_181240_0f42__-122.35006644090872_37.781019461942165.pngprediction : ship\n",
            "ship__20180705_213444_0f02__-122.31908883301435_37.70775703482464.pngprediction : ship\n",
            "ship__20171129_181545_1022__-122.34737878493225_37.730197606714256.pngprediction : ship\n",
            "ship__20171127_181539_101e__-122.34120708472079_37.71975306433755.pngprediction : ship\n",
            "ship__20171022_175534_100e__-118.2215443724113_33.73071704665499.pngprediction : ship\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}